---
title: "FPTA"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{FPTA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
### Introduction
This vignette is to show how to use the package with a toy example. “Functional Principal Trade-off Analysis” (FPTA) is a decomposition method that breaks a generic game into a sequence of simple “Disc Games” by embedding agents into two-dimensional latent spaces, each representing a distinct strategic trade-off.

We consider a game that is skew symmetric, $f(x,y) = -f(y,x)$, where $f$ is a two player symmetric, zero-sum game. The vector $x$ and $y$ represents the traits of the players, and $f$ outputs the advantage of player with traits $x$ over player with trait vector $y$. $f$ is skew-symmetric due to the symmetry of the game, i.e. the advantage of a player does not depend on the order which it is listed in the function.

Under this setting, we hope to break the game into small, analyzable and visualizable sub-games. We introduced the idea of disc game. It is a continuous analog to rock-paper-scissors in two-dimensional attribute spaces. The disc game performance function equals the cross-product between the agents' traits. In particular,
$$
    x,y \in \mathbb{R}^2, \,\,\, 
    \text{disc}(x,y) = x \times y = x_1y_2 - x_2y_1.
$$

Visually, it is the signed area of triangle formed by $x$, $y$ and origin. We hope to decompose $f$ into these smaller disc games. More specifically, 
We seek $d$ planar embeddings $Y^{(k)}:\Omega \rightarrow \mathbb{R}^2$, such that
$$
     Y^{(k)}(x) = [Y_1^{(k)}(x) \,\, , \,\, Y_2^{(k)}(x)], \\
     f(x,y) = \text{disc}[Y(x),Y(y)] = \sum_{k = 1}^d Y^{(k)}(x) \times Y^{(k)}(y).
$$
In real life scenarios, we do not know the exact form of $f$ and for $n$ different agents, we can only observe $f(x_i,x_j)$. By aligning them into a matrix, we get a skew symmetric matrix $f$. We also define a sequence of basis functions, e.g polynomial, or fourier so that we can approximate the actual $f$. 

In this toy example, I consider the function $$f(x,y) = x^2y -y^2x +x^2 -y^2 +x^3y -y^3x +x -y$$. 

Observe that this is a skew-symmetric function. We consider the realization of the function at samples points $x = (0, 1, 1.5,2)$. The realization matrice can be computed and is as follows:
### Dataset
```{r setup}
library(fpta)
f = function(x, y){
  return(x**2*y -y**2*x +x**2 -y**2 +x**3*y -y**3*x +x -y)
}
f_val = matrix(c(0,2,3.75,6, -2, 0, 4.375, 12, -3.75, -4.375, 0 ,9, -6,-12,-9,0), ncol = 4, nrow = 4)
```

In addition, we approximate $f$ using a polynomial with basis function $1, x, x^2$. The basis functions are constructed as follows:
```{r}
f1 = function(x){
  return(1.096210 + 3.116605 * x - 1.457469*x^2 )
}

f2 = function(x){
  return(1.367623 + 3.000896 * x -2.730560*x^2 )
}

x0 = function(x){
  return(1)
}
x1 = function(x){
  return(x)
}

x2 = function(x){
  return(x**2)
}


list_f = list(x0,x1,x2)
```

Now we can leverage the functions to calculate the embedding
```{r}
a = fpta::solve_embedding(f_val, list_f, c(0,1,1.5,2))
a
```
Here the function returns two outputs lambda and Mat_Embed_Coef. The value lambda represents the relative importance of each embedding. In this case, the second embedding has lambda = 0, meaning we only have exactly 1 embedding. The length of the lambda is the number of embeddings generated by the program, it is also the number of rows of Mat_Embed_Coef divided by 2. 

The Mat_Embed_Coef gives us the coefficent of each embedding. The 2i-1 row is the coefficients of the embedding for the x-coordinate using the basis function. Here it is 
```{r}
f1 = function(x){
  return(1.096210 + 3.116605 * x - 1.457469*x^2 )
}
```
Similarly the coefficient of the embedding for the y-coordinate is: 

```{r}
f2 = function(x){
  return(1.367623 + 3.000896 * x -2.730560*x^2 )
}
```

This is precisely our $Y^{(1)}(x)$ in the formula above. Therefore our estimate of the original payout function is $f_1(x)f_2(y) - f_1(y)f_2(x)$. Using this, the prediction for x = 1.2 and y = 1.5 is:
```{r}
f1(1.2)*f2(1.5) - f1(1.5)*f2(1.2)
```
This can also be calculated using the prediction formula:
```{r}
fpta::fpta_predict(list_basis = list_f, a$Mat_Embed_Coef, c(1.2), c(1.5))
```
Compare to the original payout
```{r}
f(1.2,1.5)
```
The difference mostly comes from the fact that we are approximating a degree 3 polynomial using a second degree polynomial.
Finally, one can visualize the embedding as 
```{r}
plot_embedding (list_f, a$Mat_Embed_Coef, rnorm(1000), 1)
```
The advantage of two different points can be visualized as the signed area of the triangle formed between the two points and the origin.  
